For a finite Markov Decision Process:
States:     S[t] in ğ•Š
Actions:    A[t] in ğ”¸
Rewards:    R[t],

Policy:     Ï€ : ğ•Š âŸ¶ ğ”¸
            A[t] = Ï€(S[t])

We wish to maximize future rewards (0 <= Î³ <= 1):
            R[t+1] + Î³ R[t+2] + Î³^2 R[t+3] + â€¦

Action Value Function [AVF] associated with a policy Ï€:
    q[Ï€; s, a] = ğ”¼[R[t+1] + Î³ R[t+2] + Î³^2 R[t+3] + â€¦   | S[t] == s, A[t] == a, A[t+1:âˆ] ~ Ï€]
               â‰ˆ ğ”¼[R[t+1] + Î³ q[Ï€; S[t+1], A[t+1] ]     | S[t] == s, A[t] == a, A[t+1] ~ Ï€]

An optimal policy maximizes the AVF
    q[Ï€; s, a] = max_Ï€ q[Ï€; s, a], for all s, action
(Note: all optimal policies share the same q_Ï€)

Given an optimal AVF q_Ï€, not that
    Ï€[s] = argmax_a q[Ï€; s, a] 
is an optimal policy.

Q learning:
    Take a random array Q[s, a], this will function as our best attempt at an optimal AVF
    Î”Q[ S[t], A[t] ] = Î± ( R[t+1] + Î³ max_a{ Q[ S[t+1], a ]} - Q[ S[t], A[t] ] )

ALTERNATIEF
    Î”Q[ S[t], A[t] ] = Î± ( R[t+1] + Î³ Q[ S[t+1], A[t+1] ] - Q[ S[t], A[t] ] )

